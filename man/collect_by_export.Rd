% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/extract.r
\name{collect_by_export}
\alias{collect_by_export}
\title{Collect the data by exporting into Google Cloud Storage.}
\usage{
collect_by_export(data, gs_bucket = Sys.getenv("GS_TEMP_BUCKET"),
  local_dir = tempdir(), quiet = getOption("bigquery.quiet"),
  col_types = NULL, ...)
}
\arguments{
\item{data}{Reference to the BigQuery table or query to be collected}

\item{gs_bucket}{Google Cloud Storage bucket to be
used as a temporary storage for export}

\item{local_dir}{Local directory to be used as a temporary local
storage for downloaded .csv.gz files}

\item{quiet}{if \code{FALSE}, prints informative status messages}

\item{col_types}{Types of columns (if guessed incorrectly) in the format recognized by \code{readr::read_csv}}
}
\value{
the local data.frame containing the required data
}
\description{
This is a high-level function that utilizes \code{\link{extract_exec}}
in order to export BigQuery table into Google Cloud Storage bucket,
download the resulting .csv.gz files locally, and subsequently
parse these files into a local data.frame
}
\examples{
\dontrun{
Sys.setenv(GS_TEMP_BUCKET = "<YOUR_GS_BUCKET>")

df <-
  src_bigquery(project="<PROJECT>", dataset="<DATASET>") \%>\%
  tbl("<TABLE>") \%>\%
  collect_by_export()
}
}

