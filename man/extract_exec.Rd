% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/extract.r
\name{extract_exec}
\alias{extract_exec}
\title{Run a asynchronous extract job and wait till it is done}
\usage{
extract_exec(source_table, project, destinationUris, compression = "NONE",
  destinationFormat = "CSV", fieldDelimiter = ",", printHeader = TRUE)
}
\arguments{
\item{source_table}{Source table,
either as a string in the format used by BigQuery, or as a list with
\code{project_id}, \code{dataset_id}, and \code{table_id} entries}

\item{project}{project name}

\item{destinationUris}{Specify the extract destination URI. Note: for large files, you may need to specify a wild-card since}

\item{compression}{Compression type ("NONE", "GZIP")}

\item{destinationFormat}{Destination format}

\item{fieldDelimiter}{Field delimiter (Default = ",")}

\item{printHeader}{Whether to print out a header row in the results. (Default is true)}
}
\value{
The list of integer counts of files that were produced for each destinationUri
}
\description{
This is a high-level function that inserts an extract job
(with \code{\link{insert_extract_job}}), repeatedly checks the status (with
\code{\link{get_job}}) until it is complete, then returns
}
\examples{
\dontrun{
project <- "<my_project_id>" # specify your project ID here
bucket <- "gs://<my_bucket>/shakespeare*.csv" # specify your Cloud Storage bucket name (note the wildcard)

# Now run the extract_exec - it will return the number of files that were extracted
extract_exec("publicdata:samples.shakespeare", project = project, destinationUris = bucket)
}
}
\seealso{
Google documentation for extracting data:
 \url{https://cloud.google.com/bigquery/exporting-data-from-bigquery}
}

